{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports Used",
   "id": "ee1d27379dc40cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Specify The Model Parameters",
   "id": "47f03c5b3f78abc9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_data = \"path/to/training/data.h5\"\n",
    "val_data = \"path/to/validation/data.h5\"\n",
    "folder_name = \"CatDogStandard\"\n",
    "dims = [96, 96, 1]\n",
    "num_classes = 2\n",
    "batchSize = 16\n",
    "n_epochs = 10\n",
    "lr = 0.001\n",
    "class_names = ['Cat', 'Dog']\n",
    "randomSeed = 42\n",
    "# This will continue training from previous weights\n",
    "restore_checkpoint = True"
   ],
   "id": "f60c2663c6eb77d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "e4ef8b879328391b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "np.random.seed(randomSeed)\n",
    "tf.set_random_seed(randomSeed)\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        x = f['image'][()]\n",
    "        y = f['label'][()]\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "\n",
    "x_train, y_train = load_data(train_data)\n",
    "x_val, y_val = load_data(val_data)\n",
    "\n",
    "x_train = x_train.reshape((-1, dims[0], dims[1], dims[2])).astype('float32') / 255.\n",
    "x_val = x_val.reshape((-1, dims[0], dims[1], dims[2])).astype('float32') / 255.\n",
    "y_train = y_train.astype('float32')\n",
    "y_val = y_val.astype('float32')\n",
    "\n",
    "# Shuffle the training data\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=randomSeed)\n",
    "# Shuffle the validation data\n",
    "x_val, y_val = shuffle(x_val, y_val, random_state=randomSeed)\n",
    "\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Val data shape: {x_val.shape}\")\n",
    "print(f\"Val labels shape: {y_val.shape}\")"
   ],
   "id": "397a7b8f531bf2f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Display A Sample Of The Training Data",
   "id": "f5d3eab0fc11ff94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_samples = 6\n",
    "\n",
    "plt.figure(figsize=(18, 11))\n",
    "for i in range(1, n_samples):\n",
    "    plt.subplot(1, 5, i)\n",
    "    sample_image = x_train[i]\n",
    "    plt.imshow(sample_image, cmap='gray')\n",
    "    plt.title(y_train[i])\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print('Labels:', y_train[1:n_samples])"
   ],
   "id": "5120a19fd6523bbc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define Model Architecture",
   "id": "5847154676790f3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def predictions(caps_Lminus1, dims_Lminus1, output_Lminus1,\n",
    "                caps_L, dims_L, batch_size, weight_sharing=False, grid_cells_Lminus1=1, name=None):\n",
    "    with tf.name_scope(name, default_name=\"predictions\"):\n",
    "        if weight_sharing == False:\n",
    "            output_Lminus1 = tf.squeeze(output_Lminus1, axis=[1, 4], name=\"output_Lminus1\")\n",
    "\n",
    "        init_sigma = 0.1\n",
    "        W_init = tf.random_normal(shape=(1, caps_Lminus1, caps_L, dims_L, dims_Lminus1),\n",
    "                                  stddev=init_sigma, dtype=tf.float32, name=\"W_init\")\n",
    "        W = tf.Variable(W_init, name=\"W\")\n",
    "        tf.summary.histogram(\"W\", W)\n",
    "\n",
    "        W_tiled = tf.tile(W, [batch_size, grid_cells_Lminus1, 1, 1, 1], name=\"W_tiled\")\n",
    "        output_expanded_Lminus1 = tf.expand_dims(output_Lminus1, -1, name=\"output_expanded_Lminus1\")\n",
    "        output_tile_Lminus1 = tf.expand_dims(output_expanded_Lminus1, 2, name=\"output_tile_Lminus1\")\n",
    "        output_tiled_Lminus1 = tf.tile(output_tile_Lminus1, [1, 1, caps_L, 1, 1], name=\"output_tiled_Lminus1\")\n",
    "        predicted_L = tf.matmul(W_tiled, output_tiled_Lminus1, name=\"predicted_L\")\n",
    "\n",
    "        return predicted_L\n",
    "\n",
    "def routing(caps_Lminus1, caps_L, batch_size, iterations, predicted_L, name=None):\n",
    "    with tf.name_scope(name, default_name=\"routing\"):\n",
    "        raw_weights = tf.zeros([batch_size, caps_Lminus1, caps_L, 1, 1], dtype=np.float32, name=\"raw_weights\")\n",
    "\n",
    "        for r in range(0, iterations):\n",
    "            routing_weights = tf.nn.softmax(raw_weights, dim=2, name=\"routing_weights\")\n",
    "            weighted_predictions = tf.multiply(routing_weights, predicted_L, name=\"weighted_predictions\")\n",
    "            weighted_sum = tf.reduce_sum(weighted_predictions, axis=(1), keepdims=True, name=\"weighted_sum\")\n",
    "            output_L = squash(weighted_sum, axis=-2, name=\"output_L\")\n",
    "            output_tiled_L = tf.tile(output_L, [1, caps_Lminus1, 1, 1, 1], name=\"output_tiled_L\")\n",
    "            agreement = tf.matmul(predicted_L, output_tiled_L, transpose_a=True, name=\"agreement\")\n",
    "            raw_weights = tf.add(raw_weights, agreement, name=\"raw_weights\")\n",
    "\n",
    "        return output_L, routing_weights\n",
    "\n",
    "def squash(s, axis=-1, epsilon=1e-7, name=None):\n",
    "    with tf.name_scope(name, default_name=\"squash\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis, keepdims=True)\n",
    "        safe_norm = tf.sqrt(squared_norm + epsilon)\n",
    "        squash_factor = squared_norm / (1. + squared_norm)\n",
    "        unit_vector = s / safe_norm\n",
    "        return squash_factor * unit_vector\n",
    "\n",
    "def safe_norm(s, axis=-1, epsilon=1e-7, keep_dims=False, name=None):\n",
    "    with tf.name_scope(name, default_name=\"safe_norm\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis, keep_dims=keep_dims)\n",
    "        return tf.sqrt(squared_norm + epsilon)\n",
    "\n",
    "X = tf.placeholder(shape=[None, None, None, dims[2]], dtype=tf.float32, name=\"X\")\n",
    "tf.summary.image('X', X)\n",
    "y = tf.placeholder(shape=[None], dtype=tf.int64, name=\"y\")\n",
    "\n",
    "# Block 1\n",
    "k = 3\n",
    "s = 1\n",
    "x = tf.layers.Conv2D(128, k, s, activation='relu')(X)\n",
    "x = tf.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "\n",
    "# Block 2\n",
    "k = 3\n",
    "s = 1\n",
    "x = tf.layers.Conv2D(256, k, s, activation='relu')(x)\n",
    "x = tf.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "\n",
    "# Block 3\n",
    "k = 7\n",
    "s = 1\n",
    "convcaps_dims = 16\n",
    "convcaps_caps_types = 32\n",
    "convcaps = tf.layers.Conv2D(convcaps_dims*convcaps_caps_types, k, s, name=\"convcaps\")(x)\n",
    "tf.summary.histogram(\"convcaps\", convcaps)\n",
    "\n",
    "convcaps_grid_length = tf.shape(convcaps)[1]\n",
    "convcaps_grid_width = tf.shape(convcaps)[2]\n",
    "convcaps_grid_cells = convcaps_grid_length * convcaps_grid_width\n",
    "convcaps_caps = convcaps_caps_types * convcaps_grid_cells\n",
    "\n",
    "convcaps_reshape = tf.reshape(convcaps, [-1, convcaps_caps, convcaps_dims], name=\"convcaps_reshape\")\n",
    "convcaps_output = squash(convcaps_reshape, name=\"convcaps_output\")\n",
    "\n",
    "batch_size = tf.shape(X)[0]\n",
    "\n",
    "# If doing more than 1 capsule layer caps1_caps doesn't need to be the num_classes\n",
    "caps1_caps = num_classes\n",
    "caps1_dims = 12\n",
    "\n",
    "caps1_predictions = predictions(convcaps_caps_types, convcaps_dims, convcaps_output,\n",
    "                                caps1_caps, caps1_dims, batch_size, True, convcaps_grid_cells, \"caps1_predictions\")\n",
    "\n",
    "caps1_output, routing1 = routing(convcaps_caps, caps1_caps,\n",
    "                                 batch_size, 6, caps1_predictions, \"routing1\")\n",
    "\n",
    "# You Can Add More Capsule Layers By Doing:\n",
    "# Adding a second layer:\n",
    "# -----------------------\n",
    "# caps2_caps = 8\n",
    "# caps2_dims = 16\n",
    "# \n",
    "# caps2_predictions = predictions(caps1_caps, caps1_dims, caps1_output, \n",
    "#                                 caps2_caps, caps2_dims, batch_size, False, 1, \"caps2_predictions\")\n",
    "#                                                                       ^ remains as False\n",
    "# caps2_output, routing2 = routing(caps1_caps, caps2_caps, batch_size, 9, caps2_predictions, \"routing2\")\n",
    "# \n",
    "# Adding a third layer:\n",
    "# -----------------------\n",
    "# caps3_caps = num_classes\n",
    "# caps3_dims = 16\n",
    "# \n",
    "# caps3_predictions = predictions(caps2_caps, caps2_dims, caps2_output, \n",
    "#                                 caps3_caps, caps3_dims, batch_size, False, 1, \"caps3_predictions\")\n",
    "# \n",
    "# \n",
    "# caps3_output, routing3 = routing(caps2_caps, caps3_caps, batch_size, 9, caps3_predictions, \"routing3\")\n",
    "# \n",
    "# Then Modify The Variable Below As Needed\n",
    "\n",
    "# This Line Is Here To Make Using More Capsules Work Without Modifying Variables Below!\n",
    "final_capsule_output, final_routing_output = caps1_output, routing1\n",
    "\n",
    "with tf.name_scope(name=\"accuracy_cell\"):\n",
    "    lengths_temp = safe_norm(final_capsule_output, axis=-2, name=\"lengths_temp\")\n",
    "    lengths = tf.reshape(lengths_temp, shape=(-1, num_classes), name=\"lengths\")\n",
    "    lengths_argmax = tf.argmax(lengths, axis=1, name=\"lengths_argmax\")\n",
    "    y_predictions = lengths_argmax\n",
    "    correct = tf.equal(y, y_predictions, name=\"correct\")\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "m_plus = 0.9\n",
    "m_minus = 0.1\n",
    "lambda_ = 0.5\n",
    "\n",
    "with tf.name_scope(name=\"loss_cell\"):\n",
    "    T = tf.one_hot(y, depth=caps1_caps, name=\"T\")\n",
    "    # Compute norm of each capsule in digitcaps\n",
    "    caps1_output_norm = safe_norm(final_capsule_output, axis=-2, keep_dims=True, name=\"caps1_output_norm\")\n",
    "\n",
    "    present_error_raw = tf.square(tf.maximum(0., m_plus - caps1_output_norm), name=\"present_error_raw\")\n",
    "    present_error = tf.reshape(present_error_raw, shape=(-1, caps1_caps), name=\"present_error\")\n",
    "    # -1 tells reshape to calculate the size of this dimension.\n",
    "\n",
    "    absent_error_raw = tf.square(tf.maximum(0., caps1_output_norm - m_minus), name=\"absent_error_raw\")\n",
    "    absent_error = tf.reshape(absent_error_raw, shape=(-1, caps1_caps), name=\"absent_error\")\n",
    "    # -1 tells reshape to calculate the size of this dimension.\n",
    "\n",
    "    # Compute Margin Loss\n",
    "    L = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error, name=\"L\")\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(L, axis=1), name=\"loss\")\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "# ### Adam Optimizer\n",
    "# Initialize adam optimizer to minimize $\\texttt{loss}$.\n",
    "with tf.name_scope(name=\"train\"):\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss, name=\"training_op\")"
   ],
   "id": "61af85e987b6eb2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "merged_summary = tf.summary.merge_all()\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Save model and tensorboard data to this folder\n",
    "folder = folder_name + str(batchSize) + \"_lr\" + str(lr)\n",
    "\n",
    "# Load time step for Tensorboard.\n",
    "if os.path.isfile('./tensorboard/Models/' + folder + '/time_step.pickle'):\n",
    "    time_step_unpickle = open('./tensorboard/Models/' + folder + '/time_step.pickle', 'rb')\n",
    "    time_step = pickle.load(time_step_unpickle)\n",
    "else:\n",
    "    time_step = 0\n",
    "\n",
    "# Load learning rate\n",
    "if os.path.isfile('./tensorboard/Models/' + folder + '/learning_r.pickle'):\n",
    "    learning_r_unpickle = open('./tensorboard/Models/' + folder + '/learning_r.pickle', 'rb')\n",
    "    lr = pickle.load(learning_r_unpickle)\n",
    "\n",
    "print('time step: ', time_step)\n",
    "print('learning rate: ', lr)"
   ],
   "id": "c353bd94b1319ed7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# This cell begins training the model",
   "id": "7193e3b50695f49b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_iterations_per_epoch = len(x_train) // batch_size\n",
    "n_iterations_validation = len(x_val) // batch_size\n",
    "best_loss_val = np.infty\n",
    "\n",
    "if not os.path.exists(\"./tensorboard/Models/\" + folder):\n",
    "    os.makedirs(\"./tensorboard/Models/\" + folder)\n",
    "checkpoint_path = \"./tensorboard/Models/\" + folder + \"/my_capsule_network\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if restore_checkpoint and tf.train.checkpoint_exists(checkpoint_path):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "    writer = tf.summary.FileWriter(\"./tensorboard/\" + folder)\n",
    "    writer.add_graph(sess.graph, global_step=time_step)\n",
    "\n",
    "    lr_count = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(1, n_iterations_per_epoch + 1):  # [:300]\n",
    "            X_batch = x_train[(batch_size * (iteration - 1)):(batch_size * iteration)]\n",
    "            y_batch = y_train[(batch_size * (iteration - 1)):(batch_size * iteration)]\n",
    "\n",
    "            # Every 5 iterations display loss\n",
    "            if time_step % 5 == 0:\n",
    "                loss_, merged_summary_, y_predictions_, y_ = sess.run(\n",
    "                    [loss, merged_summary, y_predictions, y],\n",
    "                    feed_dict={X: X_batch.reshape([-1, dims[0], dims[1], dims[2]]),\n",
    "                               y: y_batch,\n",
    "                               learning_rate: learning_r})\n",
    "                writer.add_summary(merged_summary_, time_step)\n",
    "                print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.6f}  Prediction: {}  y: {}\".format(\n",
    "                    iteration, n_iterations_per_epoch, iteration * 100 / n_iterations_per_epoch,\n",
    "                    loss_, y_predictions_, y_), end=\"\")\n",
    "\n",
    "            # Train Model\n",
    "            _ = sess.run([training_op], feed_dict={X: X_batch.reshape([-1, dims[0], dims[1], dims[2]]),\n",
    "                                                   y: y_batch,\n",
    "                                                   learning_rate: learning_r})\n",
    "\n",
    "            time_step = time_step + 1\n",
    "\n",
    "        # At the end of each epoch,\n",
    "        # measure the validation loss and accuracy:\n",
    "        loss_vals = []\n",
    "        acc_vals = []\n",
    "        for iteration in range(1, n_iterations_validation + 1):\n",
    "            X_batch = x_val[(batch_size * (iteration - 1)):(batch_size * iteration)]\n",
    "            y_batch = y_val[(batch_size * (iteration - 1)):(batch_size * iteration)]\n",
    "            loss_val, acc_val = sess.run(\n",
    "                [loss, accuracy],\n",
    "                feed_dict={X: X_batch.reshape([-1, dims[0], dims[1], dims[2]]),\n",
    "                           y: y_batch})\n",
    "            loss_vals.append(loss_val)\n",
    "            acc_vals.append(acc_val)\n",
    "            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                iteration, n_iterations_validation,\n",
    "                iteration * 100 / n_iterations_validation),\n",
    "                end=\" \" * 10)\n",
    "        loss_val = np.mean(loss_vals)\n",
    "        acc_val = np.mean(acc_vals)\n",
    "        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(\n",
    "            epoch + 1, acc_val * 100, loss_val,\n",
    "            \" (improved)\" if loss_val < best_loss_val else \"\"))\n",
    "\n",
    "        # And save the model if it improved:\n",
    "        if loss_val < best_loss_val:\n",
    "            save_path = saver.save(sess, checkpoint_path)\n",
    "            best_loss_val = loss_val\n",
    "        else:\n",
    "            lr_count = lr_count + 1\n",
    "\n",
    "            if lr_count == 3:\n",
    "                learning_r = learning_r / 10\n",
    "                lr_count = 0\n",
    "                print(learning_r)\n",
    "\n",
    "                with open('./tensorboard/Models/' + folder + '/learning_r.pickle', 'wb') as f:\n",
    "                    pickle.dump(learning_r, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                if restore_checkpoint and tf.train.checkpoint_exists(checkpoint_path):\n",
    "                    saver.restore(sess, checkpoint_path)\n",
    "\n",
    "        with open('./tensorboard/Models/' + folder + '/time_step.pickle', 'wb') as f:\n",
    "            pickle.dump(time_step, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "id": "897cfd71a6e40454"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluate Model On Test Data",
   "id": "19b16f778305b76e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_data = \"path/to/training/data.h5\"\n",
    "\n",
    "x_test, y_test = load_data(test_data)\n",
    "\n",
    "x_test = x_test.reshape((-1, dims[0], dims[1], dims[2])).astype('float32') / 255.\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "n_iterations_test = len(x_test) // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "\n",
    "    loss_tests = []\n",
    "    acc_tests = []\n",
    "    for iteration in range(1, n_iterations_test + 1):\n",
    "        X_batch, y_batch = x_test[\n",
    "                           (batch_size * (iteration - 1)):(batch_size * iteration)], y_test[\n",
    "                                                                                     (batch_size * (\n",
    "                                                                                                 iteration - 1)):(\n",
    "                                                                                             batch_size * iteration)]\n",
    "        loss_test, acc_test = sess.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={X: X_batch.reshape([-1, dims[0], dims[1], dims[2]]),\n",
    "                       y: y_batch})\n",
    "        loss_tests.append(loss_test)\n",
    "        acc_tests.append(acc_test)\n",
    "        print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "            iteration, n_iterations_test,\n",
    "            iteration * 100 / n_iterations_test),\n",
    "            end=\" \" * 10)\n",
    "    loss_test = np.mean(loss_tests)\n",
    "    acc_test = np.mean(acc_tests)\n",
    "    print(\"\\rFinal test accuracy: {:.4f}%  Loss: {:.6f}\".format(\n",
    "        acc_test * 100, loss_test))"
   ],
   "id": "4d169e2d5098a39"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
